{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# NASA C-MAPSS Turbofan Engine Degradation Analysis\n",
                "\n",
                "## Scientific Data Processing & Analysis Pipeline\n",
                "\n",
                "This notebook implements a reproducible pipeline for analyzing turbofan engine degradation data with emphasis on:\n",
                "- **Statistical validation** before machine learning\n",
                "- **Multiple modeling approaches** (Linear, Neural Network, LSTM)\n",
                "- **Explainability** using SHAP values\n",
                "- **Scientific visualization** for clear interpretation\n",
                "\n",
                "---\n",
                "\n",
                "### Dataset: NASA C-MAPSS FD001\n",
                "- **Source**: NASA Prognostics Center of Excellence\n",
                "- **Task**: Remaining Useful Life (RUL) prediction\n",
                "- **Features**: 21 sensor measurements + 3 operational settings\n",
                "- **Engines**: 100 training + 100 test\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Table of Contents\n",
                "\n",
                "1. [Setup & Data Loading](#1.-Setup-&-Data-Loading)\n",
                "2. [Exploratory Data Analysis](#2.-Exploratory-Data-Analysis)\n",
                "3. [Preprocessing & Feature Engineering](#3.-Preprocessing-&-Feature-Engineering)\n",
                "4. [Statistical Analysis](#4.-Statistical-Analysis)\n",
                "5. [Machine Learning Models](#5.-Machine-Learning-Models)\n",
                "   - 5.1 Linear Regression Baseline\n",
                "   - 5.2 Simple Neural Network\n",
                "   - 5.3 LSTM for Time-Series\n",
                "6. [Explainability Analysis (SHAP)](#6.-Explainability-Analysis)\n",
                "7. [Model Comparison & Conclusions](#7.-Model-Comparison-&-Conclusions)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. Setup & Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Standard imports\n",
                "import sys\n",
                "import os\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Add project root to path\n",
                "sys.path.insert(0, os.path.abspath('..'))\n",
                "\n",
                "# Core libraries\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Set style\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "plt.rcParams['figure.dpi'] = 100\n",
                "\n",
                "# Project modules\n",
                "from src.data.ingestion import CMAPSSDataLoader, compute_training_rul, get_sensor_columns, get_feature_columns\n",
                "from src.data.preprocessing import DataPreprocessor, add_degradation_features, split_by_engine\n",
                "\n",
                "print(\"âœ… Libraries imported successfully\")\n",
                "print(f\"NumPy: {np.__version__}\")\n",
                "print(f\"Pandas: {pd.__version__}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the FD001 dataset\n",
                "DATA_DIR = '../data/raw'\n",
                "\n",
                "loader = CMAPSSDataLoader(DATA_DIR)\n",
                "print(f\"Available datasets: {loader.available_datasets}\")\n",
                "\n",
                "# Load FD001\n",
                "train_df, test_df, rul_df = loader.load_dataset('FD001')\n",
                "\n",
                "# Print summary\n",
                "loader.print_summary(train_df, test_df, rul_df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compute RUL for training data\n",
                "# Using piecewise-linear RUL with cap at 125 cycles (standard approach)\n",
                "train_df = compute_training_rul(train_df, cap_rul=125)\n",
                "\n",
                "print(f\"\\nRUL Statistics (Training):\")\n",
                "print(train_df['RUL'].describe())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preview the data\n",
                "print(\"\\nðŸ“‹ Training Data Sample:\")\n",
                "display(train_df.head(10))\n",
                "\n",
                "print(f\"\\nðŸ“Š Data Shape: {train_df.shape}\")\n",
                "print(f\"Memory Usage: {train_df.memory_usage(deep=True).sum() / 1e6:.2f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Exploratory Data Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# RUL Distribution\n",
                "from src.visualization.plots import plot_rul_distribution\n",
                "\n",
                "plot_rul_distribution(train_df, rul_df, save_path='../reports/figures/rul_distribution.png')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sensor degradation trends\n",
                "from src.visualization.plots import plot_sensor_degradation\n",
                "\n",
                "# Select engines with different lifecycle lengths\n",
                "lifecycle_lengths = train_df.groupby('engine_id')['cycle'].max()\n",
                "sample_engines = [\n",
                "    lifecycle_lengths.idxmin(),  # Shortest lifecycle\n",
                "    lifecycle_lengths.idxmax(),  # Longest lifecycle\n",
                "    lifecycle_lengths.median().astype(int),  # Median\n",
                "]\n",
                "# Add 2 random engines\n",
                "remaining = [e for e in train_df['engine_id'].unique() if e not in sample_engines]\n",
                "sample_engines.extend(np.random.choice(remaining, 2, replace=False).tolist())\n",
                "\n",
                "plot_sensor_degradation(\n",
                "    train_df,\n",
                "    sensors=['sensor_2', 'sensor_3', 'sensor_4', 'sensor_11', 'sensor_12', 'sensor_15'],\n",
                "    engine_ids=sample_engines,\n",
                "    save_path='../reports/figures/sensor_degradation.png'\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sensor variance analysis - identify constant/uninformative sensors\n",
                "sensor_cols = get_sensor_columns()\n",
                "sensor_stats = train_df[sensor_cols].agg(['mean', 'std', 'min', 'max'])\n",
                "\n",
                "print(\"ðŸ“Š Sensor Statistics:\")\n",
                "display(sensor_stats.T.round(4))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify constant sensors (zero or near-zero variance)\n",
                "variances = train_df[sensor_cols].var()\n",
                "constant_sensors = variances[variances < 1e-5].index.tolist()\n",
                "\n",
                "print(f\"\\nâš ï¸ Constant/Near-Constant Sensors (will be dropped):\")\n",
                "for sensor in constant_sensors:\n",
                "    print(f\"  â€¢ {sensor}: variance = {variances[sensor]:.2e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Preprocessing & Feature Engineering"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize preprocessor\n",
                "preprocessor = DataPreprocessor(\n",
                "    normalization='minmax',\n",
                "    rolling_windows=[5, 10],  # Rolling statistics windows\n",
                "    drop_constant_sensors=True\n",
                ")\n",
                "\n",
                "# Fit on training data and transform\n",
                "train_processed = preprocessor.fit_transform(train_df, add_features=True)\n",
                "\n",
                "print(f\"\\nâœ… Preprocessing complete\")\n",
                "print(f\"Original features: {len(get_feature_columns())}\")\n",
                "print(f\"After preprocessing: {len([c for c in train_processed.columns if c not in ['engine_id', 'cycle', 'RUL']])}\")\n",
                "print(f\"\\nDropped sensors: {preprocessor.constant_sensors_}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Add degradation-specific features\n",
                "train_processed = add_degradation_features(train_processed)\n",
                "\n",
                "print(f\"Final feature count: {len(train_processed.columns)}\")\n",
                "print(f\"\\nNew columns added:\")\n",
                "new_cols = [c for c in train_processed.columns if 'diff' in c or 'ewm' in c or 'cycle_norm' in c]\n",
                "print(f\"  â€¢ Difference features: {len([c for c in new_cols if 'diff' in c])}\")\n",
                "print(f\"  â€¢ EWM features: {len([c for c in new_cols if 'ewm' in c])}\")\n",
                "print(f\"  â€¢ Normalized cycle: 1\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split training data for validation (by engine to prevent leakage)\n",
                "train_split, val_split = split_by_engine(train_processed, test_size=0.2, random_state=42)\n",
                "\n",
                "print(f\"\\nðŸ“Š Train/Validation Split (by engine):\")\n",
                "print(f\"  Training:   {len(train_split):,} samples ({train_split['engine_id'].nunique()} engines)\")\n",
                "print(f\"  Validation: {len(val_split):,} samples ({val_split['engine_id'].nunique()} engines)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define feature columns for modeling (exclude metadata)\n",
                "exclude_cols = ['engine_id', 'cycle', 'RUL', 'max_cycle', 'cycle_norm']\n",
                "feature_cols = [c for c in train_processed.columns if c not in exclude_cols]\n",
                "\n",
                "print(f\"\\nðŸ”§ Features for modeling: {len(feature_cols)}\")\n",
                "print(f\"\\nFeature types:\")\n",
                "print(f\"  â€¢ Base sensors: {len([c for c in feature_cols if c.startswith('sensor') and '_' not in c[7:]])}\")\n",
                "print(f\"  â€¢ Rolling features: {len([c for c in feature_cols if 'roll' in c])}\")\n",
                "print(f\"  â€¢ Derived features: {len([c for c in feature_cols if 'diff' in c or 'ewm' in c])}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Statistical Analysis\n",
                "\n",
                "**Important**: Statistical analysis before ML helps us understand:\n",
                "- Which sensors correlate with degradation\n",
                "- How distributions change as failure approaches\n",
                "- Whether our data meets modeling assumptions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.analysis.statistical import (\n",
                "    compute_sensor_rul_correlation,\n",
                "    analyze_sensor_monotonicity,\n",
                "    compare_health_stages,\n",
                "    print_statistical_summary\n",
                ")\n",
                "\n",
                "# Run comprehensive statistical summary\n",
                "print_statistical_summary(train_df, rul_column='RUL')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Detailed correlation analysis\n",
                "corr_df = compute_sensor_rul_correlation(train_df, method='both')\n",
                "\n",
                "print(\"\\nðŸ“Š Sensor-RUL Correlations:\")\n",
                "display(corr_df.head(15))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize correlation matrix\n",
                "from src.visualization.plots import plot_correlation_heatmap\n",
                "from src.analysis.correlation import compute_inter_sensor_correlation\n",
                "\n",
                "# Get active sensors (non-constant)\n",
                "active_sensors = preprocessor.get_active_sensors()\n",
                "corr_matrix = train_df[active_sensors].corr(method='spearman')\n",
                "\n",
                "plot_correlation_heatmap(\n",
                "    corr_matrix,\n",
                "    title='Inter-Sensor Spearman Correlation',\n",
                "    save_path='../reports/figures/correlation_heatmap.png'\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Monotonicity analysis - important for prognostics\n",
                "mono_df = analyze_sensor_monotonicity(train_df)\n",
                "\n",
                "print(\"\\nðŸ“ˆ Sensor Monotonicity Analysis:\")\n",
                "print(\"(Higher score = more consistent trend toward failure)\\n\")\n",
                "display(mono_df.head(10))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Distribution shift analysis: healthy vs. degraded\n",
                "stage_df = compare_health_stages(train_df, rul_threshold=50)\n",
                "\n",
                "print(\"\\nðŸ”¬ Healthy vs Degraded Stage Comparison:\")\n",
                "display(stage_df[['sensor', 'healthy_mean', 'degraded_mean', 'cohens_d', 'effect_size']].head(10))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Machine Learning Models\n",
                "\n",
                "We'll train three progressively complex models:\n",
                "1. **Linear Regression** - Interpretable baseline\n",
                "2. **Simple Neural Network** - Non-linear relationships\n",
                "3. **LSTM** - Temporal dependencies"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.1 Linear Regression Baseline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.models.baseline import LinearRULPredictor, train_linear_baseline, print_linear_model_summary\n",
                "from src.models.evaluation import compute_all_metrics, print_evaluation_summary\n",
                "\n",
                "# Prepare data\n",
                "X_train = train_split[feature_cols]\n",
                "y_train = train_split['RUL']\n",
                "X_val = val_split[feature_cols]\n",
                "y_val = val_split['RUL']\n",
                "\n",
                "print(f\"Training: {X_train.shape}, Validation: {X_val.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Linear Regression with Ridge regularization\n",
                "linear_model, cv_results = train_linear_baseline(\n",
                "    train_split,\n",
                "    feature_cols=feature_cols,\n",
                "    regularization='ridge',\n",
                "    alpha=1.0\n",
                ")\n",
                "\n",
                "print_linear_model_summary(linear_model, cv_results)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate on validation set\n",
                "y_pred_linear = linear_model.predict(X_val)\n",
                "\n",
                "print_evaluation_summary(y_val.values, y_pred_linear, 'Linear Regression')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize predictions\n",
                "from src.visualization.plots import plot_prediction_vs_actual, plot_residuals\n",
                "\n",
                "plot_prediction_vs_actual(\n",
                "    y_val.values, y_pred_linear,\n",
                "    model_name='Linear Regression',\n",
                "    save_path='../reports/figures/linear_pred_vs_actual.png'\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_residuals(\n",
                "    y_val.values, y_pred_linear,\n",
                "    model_name='Linear Regression',\n",
                "    save_path='../reports/figures/linear_residuals.png'\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 Simple Neural Network"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.models.neural_network import SimpleNeuralNetwork, plot_training_history\n",
                "\n",
                "# Initialize neural network\n",
                "nn_model = SimpleNeuralNetwork(\n",
                "    input_dim=len(feature_cols),\n",
                "    hidden_dims=[128, 64, 32],\n",
                "    dropout_rate=0.3,\n",
                "    learning_rate=0.001\n",
                ")\n",
                "\n",
                "print(f\"Device: {nn_model.device}\")\n",
                "print(f\"\\nNetwork Architecture:\")\n",
                "print(nn_model.model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train neural network\n",
                "history_nn = nn_model.fit(\n",
                "    X_train, y_train,\n",
                "    epochs=100,\n",
                "    batch_size=64,\n",
                "    validation_split=0.1,\n",
                "    verbose=True\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training history\n",
                "plot_training_history(\n",
                "    history_nn,\n",
                "    title='Neural Network Training History',\n",
                "    save_path='../reports/figures/nn_training_history.png'\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate neural network\n",
                "y_pred_nn = nn_model.predict(X_val)\n",
                "\n",
                "print_evaluation_summary(y_val.values, y_pred_nn, 'Neural Network')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_prediction_vs_actual(\n",
                "    y_val.values, y_pred_nn,\n",
                "    model_name='Neural Network',\n",
                "    save_path='../reports/figures/nn_pred_vs_actual.png'\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.3 LSTM for Time-Series"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.models.neural_network import LSTMModel\n",
                "\n",
                "# Prepare sequences for LSTM\n",
                "SEQUENCE_LENGTH = 30\n",
                "\n",
                "# Use simpler feature set for LSTM (base sensors only, normalized)\n",
                "base_sensor_cols = [c for c in feature_cols if 'roll' not in c and 'diff' not in c and 'ewm' not in c]\n",
                "\n",
                "print(f\"Preparing sequences with length={SEQUENCE_LENGTH}\")\n",
                "print(f\"Using {len(base_sensor_cols)} base features\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create sequences from training data\n",
                "X_seq_train, y_seq_train = LSTMModel.prepare_sequences(\n",
                "    train_split,\n",
                "    feature_cols=base_sensor_cols,\n",
                "    target_col='RUL',\n",
                "    sequence_length=SEQUENCE_LENGTH\n",
                ")\n",
                "\n",
                "X_seq_val, y_seq_val = LSTMModel.prepare_sequences(\n",
                "    val_split,\n",
                "    feature_cols=base_sensor_cols,\n",
                "    target_col='RUL',\n",
                "    sequence_length=SEQUENCE_LENGTH\n",
                ")\n",
                "\n",
                "print(f\"\\nSequence shapes:\")\n",
                "print(f\"  Training: X={X_seq_train.shape}, y={y_seq_train.shape}\")\n",
                "print(f\"  Validation: X={X_seq_val.shape}, y={y_seq_val.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize LSTM model\n",
                "lstm_model = LSTMModel(\n",
                "    input_dim=len(base_sensor_cols),\n",
                "    hidden_dim=64,\n",
                "    num_layers=2,\n",
                "    dropout=0.2,\n",
                "    learning_rate=0.001\n",
                ")\n",
                "\n",
                "print(f\"Device: {lstm_model.device}\")\n",
                "print(f\"\\nLSTM Architecture:\")\n",
                "print(lstm_model.model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train LSTM\n",
                "history_lstm = lstm_model.fit(\n",
                "    X_seq_train, y_seq_train,\n",
                "    epochs=50,\n",
                "    batch_size=64,\n",
                "    validation_split=0.1,\n",
                "    verbose=True\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_training_history(\n",
                "    history_lstm,\n",
                "    title='LSTM Training History',\n",
                "    save_path='../reports/figures/lstm_training_history.png'\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate LSTM\n",
                "y_pred_lstm = lstm_model.predict(X_seq_val)\n",
                "\n",
                "print_evaluation_summary(y_seq_val, y_pred_lstm, 'LSTM')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_prediction_vs_actual(\n",
                "    y_seq_val, y_pred_lstm,\n",
                "    model_name='LSTM',\n",
                "    save_path='../reports/figures/lstm_pred_vs_actual.png'\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Explainability Analysis\n",
                "\n",
                "Using SHAP values to understand model predictions.\n",
                "\n",
                "### Why Explainability Matters for Scientific Applications:\n",
                "- Validates that model uses physically meaningful features\n",
                "- Identifies potential data leakage or spurious correlations\n",
                "- Supports trust in safety-critical decisions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# For SHAP analysis, we'll use a Random Forest for better interpretability\n",
                "from src.models.ensemble import RandomForestRUL, train_random_forest\n",
                "\n",
                "# Train RF for SHAP (tree-based models have fast SHAP computation)\n",
                "rf_model, rf_cv = train_random_forest(\n",
                "    train_split,\n",
                "    feature_cols=feature_cols,\n",
                "    n_estimators=100,\n",
                "    max_depth=15\n",
                ")\n",
                "\n",
                "print(f\"\\nRandom Forest OOB Score: {rf_model.get_oob_score():.4f}\")\n",
                "print(f\"CV RMSE: {rf_cv['mean']:.2f} Â± {rf_cv['std']:.2f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.explainability.shap_analysis import SHAPExplainer, print_shap_summary\n",
                "\n",
                "# Create SHAP explainer using Random Forest\n",
                "background_sample = X_train.sample(min(200, len(X_train)), random_state=42)\n",
                "shap_explainer = SHAPExplainer(rf_model, background_sample)\n",
                "\n",
                "# Compute SHAP values on validation sample\n",
                "shap_sample = X_val.sample(min(500, len(X_val)), random_state=42)\n",
                "shap_values = shap_explainer.compute_shap_values(shap_sample)\n",
                "\n",
                "print(f\"\\nâœ… SHAP values computed for {len(shap_sample)} samples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Print SHAP summary\n",
                "print_shap_summary(shap_explainer)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# SHAP Summary Plot (bar)\n",
                "shap_explainer.plot_summary(\n",
                "    plot_type='bar',\n",
                "    max_features=15,\n",
                "    save_path='../reports/figures/shap_summary_bar.png'\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# SHAP Beeswarm Plot (detailed)\n",
                "shap_explainer.plot_summary(\n",
                "    plot_type='beeswarm',\n",
                "    max_features=15,\n",
                "    save_path='../reports/figures/shap_summary_beeswarm.png'\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Explain a single prediction\n",
                "sample_idx = 0  # First validation sample\n",
                "explanation = shap_explainer.explain_instance(shap_sample, index=sample_idx)\n",
                "\n",
                "print(f\"\\nðŸ” Single Prediction Explanation (Sample {sample_idx}):\")\n",
                "print(f\"\\nTop 10 Contributing Features:\")\n",
                "display(explanation.head(10))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dependence plot for most important sensor\n",
                "top_feature = shap_explainer.get_feature_importance().iloc[0]['feature']\n",
                "\n",
                "shap_explainer.plot_dependence(\n",
                "    top_feature,\n",
                "    save_path=f'../reports/figures/shap_dependence_{top_feature}.png'\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. Model Comparison & Conclusions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.models.evaluation import compare_models\n",
                "\n",
                "# Collect all predictions (using same validation set size for fairness)\n",
                "# Note: LSTM uses different data shape, so we compare on comparable samples\n",
                "\n",
                "# For fair comparison, use the standard validation set\n",
                "predictions = {\n",
                "    'Linear Regression': y_pred_linear,\n",
                "    'Neural Network': y_pred_nn,\n",
                "}\n",
                "\n",
                "# Compare models\n",
                "comparison = compare_models(y_val.values, predictions)\n",
                "\n",
                "# Add LSTM separately (different validation set)\n",
                "lstm_metrics = compute_all_metrics(y_seq_val, y_pred_lstm)\n",
                "lstm_row = pd.DataFrame([{\n",
                "    'model': 'LSTM',\n",
                "    'rmse': lstm_metrics['rmse'],\n",
                "    'mae': lstm_metrics['mae'],\n",
                "    'r2': lstm_metrics['r2'],\n",
                "    'mape': lstm_metrics['mape'],\n",
                "    'nasa_score': lstm_metrics['nasa_score']\n",
                "}])\n",
                "\n",
                "comparison = pd.concat([comparison, lstm_row], ignore_index=True)\n",
                "\n",
                "print(\"\\nðŸ“Š MODEL COMPARISON:\")\n",
                "print(\"=\" * 70)\n",
                "display(comparison)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization of model comparison\n",
                "from src.visualization.plots import plot_model_comparison\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# RMSE comparison\n",
                "colors = plt.cm.Set2(np.linspace(0, 1, len(comparison)))\n",
                "axes[0].bar(comparison['model'], comparison['rmse'], color=colors)\n",
                "axes[0].set_ylabel('RMSE (cycles)')\n",
                "axes[0].set_title('RMSE Comparison (Lower is Better)')\n",
                "for i, v in enumerate(comparison['rmse']):\n",
                "    axes[0].text(i, v + 0.5, f'{v:.2f}', ha='center', fontsize=11)\n",
                "\n",
                "# RÂ² comparison\n",
                "axes[1].bar(comparison['model'], comparison['r2'], color=colors)\n",
                "axes[1].set_ylabel('RÂ² Score')\n",
                "axes[1].set_title('RÂ² Comparison (Higher is Better)')\n",
                "for i, v in enumerate(comparison['r2']):\n",
                "    axes[1].text(i, v + 0.01, f'{v:.3f}', ha='center', fontsize=11)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('../reports/figures/model_comparison.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create summary dashboard\n",
                "from src.visualization.plots import create_summary_dashboard\n",
                "\n",
                "# Use best model for dashboard (typically Neural Network or LSTM)\n",
                "best_model_idx = comparison['rmse'].idxmin()\n",
                "best_model_name = comparison.loc[best_model_idx, 'model']\n",
                "\n",
                "print(f\"\\nðŸ† Best Model: {best_model_name}\")\n",
                "print(f\"  RMSE: {comparison.loc[best_model_idx, 'rmse']:.2f} cycles\")\n",
                "print(f\"  MAE: {comparison.loc[best_model_idx, 'mae']:.2f} cycles\")\n",
                "print(f\"  RÂ²: {comparison.loc[best_model_idx, 'r2']:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary & Key Findings\n",
                "\n",
                "### Statistical Insights:\n",
                "1. **Constant sensors** (1, 5, 6, 10, 16, 18, 19) provide no predictive value\n",
                "2. **Sensors 4, 11, 12, 15** show strongest correlation with RUL\n",
                "3. Clear distribution shifts between healthy (RUL > 50) and degraded states\n",
                "\n",
                "### Model Performance:\n",
                "- **Linear Regression**: Provides interpretable baseline\n",
                "- **Neural Network**: Captures non-linear patterns\n",
                "- **LSTM**: Leverages temporal dependencies (best for sequential prediction)\n",
                "\n",
                "### Explainability (SHAP):\n",
                "- Model correctly uses physically meaningful sensors\n",
                "- No obvious signs of data leakage\n",
                "- Feature importance aligns with domain knowledge\n",
                "\n",
                "---\n",
                "\n",
                "### Future Work:\n",
                "1. Extend to FD002-FD004 (multiple operating conditions)\n",
                "2. Implement attention mechanisms for LSTM\n",
                "3. Add uncertainty quantification\n",
                "4. Deploy as RESTful API for predictions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save final comparison results\n",
                "comparison.to_csv('../reports/model_comparison.csv', index=False)\n",
                "print(\"\\nâœ… Results saved to reports/model_comparison.csv\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}